AMGX,GH200,32x32x32,SymGS
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'236",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.65",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.65",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.75",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.53",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'511.01",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.73",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.39",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.86",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.86",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.44"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","186.84",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.65",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.88",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.76",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.62",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.73",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.38"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.736"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.81",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.19",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.95",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.95 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.19"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.05",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.05",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.83",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.47",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","788.68",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","416'423",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","837.87",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","442'396",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11403 fused and 8869 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5083"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.44",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.92",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.74"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","657.83",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","678'528",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'511.01",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'075'744",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'660.09",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","877'056",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'511.01",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'075'744",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'377.82",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'302'976",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 36.64% above the average, while the minimum instance value is 49.42% below the average.","global","15.78"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.14% above the average, while the minimum instance value is 52.63% below the average.","global","16.22"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 36.64% above the average, while the minimum instance value is 49.42% below the average.","global","15.78"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.67% above the average, while the minimum instance value is 17.96% below the average.","global","6.461"
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'509",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.80",
"0","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7785 excessive sectors (20% of the total 38969 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.19"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'993",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.90",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.84",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.34",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","7.00",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'848.52",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.99",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.82",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.40",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","21.85",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.87",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","21.85",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.77"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","193.56",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.90",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","7.07",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.98",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.15",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.99",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.487"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.922"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.36",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.64",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.23",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.23 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.64"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.44",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.48",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.80",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.44",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","791.05",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","417'673",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","840.89",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","443'990",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11493 fused and 8939 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4674"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.33",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.77",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.81"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","661.33",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","655'872",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'848.52",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'050'922",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'710.50",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","851'424",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'848.52",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'050'922",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'315.56",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'203'688",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.78% above the average, while the minimum instance value is 45.54% below the average.","global","16.81"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 38.44% above the average, while the minimum instance value is 54.13% below the average.","global","16.01"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.78% above the average, while the minimum instance value is 45.54% below the average.","global","16.81"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.12% above the average, while the minimum instance value is 15.72% below the average.","global","9.624"
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'659",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.84",
"1","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7810 excessive sectors (20% of the total 39229 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.57"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'188",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.16",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.59",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.14",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.41",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'369.66",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.19",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.77",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.77",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.94"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","183.48",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.16",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.48",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.85",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.74",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.153"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.373"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.38",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.62",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.35",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.35 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.62"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.91",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.35",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.99",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.64",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","777.08",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","410'298",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","834.78",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","440'763",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 10962 fused and 8526 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5092"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.72",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.46",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.48"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","646",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","675'072",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'369.66",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'125'930",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'738.45",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","874'752",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'369.66",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'125'930",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'289.72",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'503'720",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.15% above the average, while the minimum instance value is 46.17% below the average.","global","15.47"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.01% above the average, while the minimum instance value is 51.33% below the average.","global","16.98"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.15% above the average, while the minimum instance value is 46.17% below the average.","global","15.47"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 30.75% above the average, while the minimum instance value is 17.25% below the average.","global","15.99"
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","39'774",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.61",
"2","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7696 excessive sectors (20% of the total 38040 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.52"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'056",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.42",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.72",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.67",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.81",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'498.36",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.42",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.38",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.90",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.90",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.48"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","188.63",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.42",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.67",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.22",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.39",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.42",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.263"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.565"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.55",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.45",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.04",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.50",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.04 active warps per scheduler, but only an average of 0.50 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.45"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.76",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.07",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.95",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.59",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","781.10",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","412'423",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","836.27",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","441'549",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11115 fused and 8645 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4973"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.97",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.62",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.22"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","652.33",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","663'552",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'498.36",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'099'546",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'636.74",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","860'640",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'498.36",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'099'546",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'405.70",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'398'184",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.67% above the average, while the minimum instance value is 48.75% below the average.","global","17.5"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.40% above the average, while the minimum instance value is 48.94% below the average.","global","16.11"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.67% above the average, while the minimum instance value is 48.75% below the average.","global","17.5"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 29.79% above the average, while the minimum instance value is 15.80% below the average.","global","15.41"
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'029",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.68",
"3","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7785 excessive sectors (20% of the total 38520 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.45"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'276",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.00",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.20",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.55",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.98",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'328.45",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.19",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.90",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.90",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.15"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","168.14",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.00",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.35",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.56",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.03",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.055"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.249"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.00",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.00",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.04",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.04 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.16",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.20",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.33",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.00",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","747.96",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","394'923",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","795.54",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","420'045",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9855 fused and 7665 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4634"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.31",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.84",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.88"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","595.50",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","680'448",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'328.45",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'101'894",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'712.19",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","881'856",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'328.45",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'101'894",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'182.62",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'407'576",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.96% above the average, while the minimum instance value is 45.89% below the average.","global","15.93"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.61% above the average, while the minimum instance value is 51.20% below the average.","global","16.63"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.96% above the average, while the minimum instance value is 45.89% below the average.","global","15.93"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 16.18% above the average, while the minimum instance value is 10.74% below the average.","global","8.302"
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'929",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.15",
"4","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6793 excessive sectors (20% of the total 34163 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.2"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'259",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.75",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.16",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.54",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.97",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'288.07",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.91",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.92",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.92",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.03"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","165.98",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.75",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.15",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.28",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.20",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.91",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.951"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.063"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.25",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.75",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.23",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.23 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.75"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.54",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","38.70",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.39",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.06",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","742.52",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","392'048",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","786.46",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","415'253",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9648 fused and 7504 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4592"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.45",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.57",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.72"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","587.83",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","677'632",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'288.07",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'127'726",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'851.01",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","879'840",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'288.07",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'127'726",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'114.70",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'510'904",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.05% above the average, while the minimum instance value is 45.23% below the average.","global","15.8"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.69% above the average, while the minimum instance value is 52.23% below the average.","global","17.02"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.05% above the average, while the minimum instance value is 45.23% below the average.","global","15.8"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 14.03% above the average, while the minimum instance value is 12.95% below the average.","global","7.429"
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'584",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.06",
"5","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6640 excessive sectors (20% of the total 33384 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.53"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'197",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.09",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.09",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.86",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.89",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'189.97",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.54",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.74",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.74",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.64"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","163.12",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.09",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.48",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.63",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.85",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.54",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.083"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.319"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.01",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.99",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.61",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.61 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.99"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.41",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.19",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.47",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.15",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","735.89",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","388'548",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","789.21",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","416'702",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9396 fused and 7308 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.461"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.48",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.30",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.72"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","574.33",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","674'560",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'189.97",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'060'406",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'612.10",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","873'792",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'189.97",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'060'406",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'155.87",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'241'624",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.77% above the average, while the minimum instance value is 45.52% below the average.","global","17.38"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.87% above the average, while the minimum instance value is 44.10% below the average.","global","16.45"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.77% above the average, while the minimum instance value is 45.52% below the average.","global","17.38"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.04% above the average, while the minimum instance value is 16.05% below the average.","global","12.69"
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'164",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.95",
"6","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6322 excessive sectors (20% of the total 32346 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.904"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'015",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.71",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.23",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.28",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.26",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.06",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'145.87",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.89",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.98",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.98",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.37"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","168.58",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.71",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.10",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.75",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.11",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.89",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.921"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.031"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.74",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.26",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.15",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.15 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.26"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.94",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.03",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.43",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.11",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","738.73",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","390'048",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","785.77",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","414'888",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9504 fused and 7392 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4728"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.22",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.78",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.97"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","579.50",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","658'176",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'145.87",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'130'504",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'745.04",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","854'496",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'145.87",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'130'504",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'176.67",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'522'016",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.67% above the average, while the minimum instance value is 44.98% below the average.","global","17.51"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.61% above the average, while the minimum instance value is 48.53% below the average.","global","17.29"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.67% above the average, while the minimum instance value is 44.98% below the average.","global","17.51"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 15.93% above the average, while the minimum instance value is 16.17% below the average.","global","8.491"
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'344",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4",
"7","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6401 excessive sectors (20% of the total 32692 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.44"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'069",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.75",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.05",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.05",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.70",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'111.97",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.08",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.08",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.08",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.37"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","162.02",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.75",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.16",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.05",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.07",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.08",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.957"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.059"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.83",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.17",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.61",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.61 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.17"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.94",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.30",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.58",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.27",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","726.65",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","383'673",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","780.44",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","412'072",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9045 fused and 7035 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4549"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.51",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.61",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.65"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","560.33",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","663'552",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'111.97",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'099'268",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'657.14",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","859'008",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'111.97",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'099'268",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'275.66",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'397'072",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.68% above the average, while the minimum instance value is 43.89% below the average.","global","16.69"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.60% above the average, while the minimum instance value is 44.77% below the average.","global","16.76"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.68% above the average, while the minimum instance value is 43.89% below the average.","global","16.69"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 20.02% above the average, while the minimum instance value is 16.73% below the average.","global","10.42"
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'579",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.81",
"8","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6240 excessive sectors (20% of the total 31250 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.39"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'120",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.68",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.05",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.59",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.81",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'227.24",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.90",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.12",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.12",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.92"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","162.54",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.68",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.07",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.43",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.51",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.90",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.949"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.014"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.63",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.37",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.62",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.62 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.37"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.24",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.40",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.55",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.23",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","729.73",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","385'298",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","778.55",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","411'073",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9162 fused and 7126 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4443"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.33",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.49",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.84"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","565.50",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","669'696",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'227.24",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'121'200",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'885.40",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","866'208",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'227.24",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'121'200",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'294.47",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'484'800",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.89% above the average, while the minimum instance value is 46.11% below the average.","global","15.92"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.14% above the average, while the minimum instance value is 45.06% below the average.","global","17.12"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.89% above the average, while the minimum instance value is 46.11% below the average.","global","15.92"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.13% above the average, while the minimum instance value is 13.58% below the average.","global","12.52"
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'774",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.86",
"9","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6541 excessive sectors (20% of the total 31954 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.08"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'302",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.78",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.86",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.47",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.56",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.41",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'993.77",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.21",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.61",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.61",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.8"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","154.01",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.78",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.21",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.25",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.12",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.21",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.99"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.087"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.46",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.54",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.28",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.28 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.54"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.51",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.59",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.65",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.35",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","720.50",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","380'423",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","766.59",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","404'758",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8811 fused and 6853 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4606"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.47",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.58",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.7"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","548.67",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","681'984",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'993.77",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'081'186",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'559.86",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","885'216",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'993.77",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'081'186",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'010.96",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'324'744",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.94% above the average, while the minimum instance value is 43.22% below the average.","global","16.06"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 45.59% above the average, while the minimum instance value is 45.53% below the average.","global","16.76"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.94% above the average, while the minimum instance value is 43.22% below the average.","global","16.06"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.08% above the average, while the minimum instance value is 21.97% below the average.","global","12.9"
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'189",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.71",
"10","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6164 excessive sectors (20% of the total 30483 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'412",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.67",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.93",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.54",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.89",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.59",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'146.36",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.96",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.86",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.86",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.52"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","156.58",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.67",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.10",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.50",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.02",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.96",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.92"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.005"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.25",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.75",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.35",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.35 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.75"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.09",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.54",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.57",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.25",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","728.31",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","384'548",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","782.29",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","413'049",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9108 fused and 7084 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4531"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.91",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.22",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.27"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","564.33",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","689'152",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'146.36",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'113'710",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'696.73",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","897'504",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'146.36",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'113'710",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'097.87",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'454'840",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.40% above the average, while the minimum instance value is 43.20% below the average.","global","15.81"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.66% above the average, while the minimum instance value is 50.19% below the average.","global","17.13"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.40% above the average, while the minimum instance value is 43.20% below the average.","global","15.81"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.28% above the average, while the minimum instance value is 19.82% below the average.","global","13.2"
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'684",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.83",
"11","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6300 excessive sectors (20% of the total 31557 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.03"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'053",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.41",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.78",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.28",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.73",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.50",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'056.55",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.80",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.41",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.41",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.36"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","152",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.41",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.88",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.98",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.27",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.80",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.808"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.807"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.48",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.52",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.79",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.79 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.52"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.83",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.59",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.84",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.54",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","707.00",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","373'298",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","746.17",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","393'979",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8298 fused and 6454 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4249"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.13",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.00",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.03"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","522.50",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","663'040",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'056.55",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'116'190",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'494.47",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","860'640",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'056.55",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'116'190",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'047.90",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'464'760",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.74% above the average, while the minimum instance value is 43.37% below the average.","global","16.9"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 47.33% above the average, while the minimum instance value is 46.03% below the average.","global","17.06"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.74% above the average, while the minimum instance value is 43.37% below the average.","global","16.9"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.40% above the average, while the minimum instance value is 21.99% below the average.","global","11.23"
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'334",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.49",
"12","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5767 excessive sectors (20% of the total 28779 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.05"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'854",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.61",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.01",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.15",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.63",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.64",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'134.73",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.02",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.16",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.97",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.16",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.63"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","160.70",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.61",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.04",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.46",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.96",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.02",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.906"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.955"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.62",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.38",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.63",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.63 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.38"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.29",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.13",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.71",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.41",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 35.8% of the total average of 32.3 cycles between issuing two instructions.","global","35.8"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","716.71",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","378'423",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","757.50",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","399'959",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8667 fused and 6741 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4327"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.71",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.38",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.43"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","539",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","645'120",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'134.73",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'103'904",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'665.44",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","838'272",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'134.73",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'103'904",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'206.85",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'415'616",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.09% above the average, while the minimum instance value is 44.46% below the average.","global","16.15"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.74% above the average, while the minimum instance value is 41.66% below the average.","global","16.77"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.09% above the average, while the minimum instance value is 44.46% below the average.","global","16.15"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.92% above the average, while the minimum instance value is 14.46% below the average.","global","10.11"
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'949",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.65",
"13","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6132 excessive sectors (20% of the total 30180 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.86"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.58",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'193",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.39",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.74",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.23",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.20",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'147.29",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.75",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.92",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.92",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.86"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","148.54",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.39",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.86",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.37",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.65",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.75",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.822"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.793"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.64",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.36",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.65",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.65 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.36"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.38",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.55",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.87",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.58",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","705.58",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'548",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","752.84",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","397'498",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8244 fused and 6412 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.41"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.47",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.50",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.65"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","523",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","670'720",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'147.29",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'119'842",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'642.16",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","873'696",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'147.29",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'119'842",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'184.32",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'479'368",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 44.27% above the average, while the minimum instance value is 40.33% below the average.","global","16.42"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.37% above the average, while the minimum instance value is 43.10% below the average.","global","17.41"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 44.27% above the average, while the minimum instance value is 40.33% below the average.","global","16.42"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.57% above the average, while the minimum instance value is 17.11% below the average.","global","11"
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'244",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.47",
"14","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6040 excessive sectors (21% of the total 29046 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.61"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'134",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.82",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.78",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.00",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.31",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'198.61",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.41",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.64",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.64",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.07"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","150.85",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.82",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.26",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.35",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.59",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.41",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.99"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.114"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.90",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.10",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.71",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.71 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.1"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.63",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.85",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.80",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.51",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.7 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 31.7% of the total average of 33.6 cycles between issuing two instructions.","global","31.7"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","709.37",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","374'548",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","756.10",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","399'221",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8388 fused and 6524 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4104"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.17",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.31",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.96"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","524.83",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","665'856",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'198.61",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'052'730",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'538.24",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","867'360",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'198.61",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'052'730",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'919.43",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'210'920",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 44.70% above the average, while the minimum instance value is 39.91% below the average.","global","17.93"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.90% above the average, while the minimum instance value is 43.52% below the average.","global","16.8"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 44.70% above the average, while the minimum instance value is 39.91% below the average.","global","17.93"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.45% above the average, while the minimum instance value is 18.32% below the average.","global","12.28"
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'484",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.53",
"15","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5809 excessive sectors (20% of the total 29021 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.05"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'151",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.76",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.73",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.91",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.17",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'013.41",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.36",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.74",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.74",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.16"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","148.71",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.76",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.21",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.24",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.76",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.36",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.985"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.068"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.96",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.04",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.47",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.47 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.04"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.32",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.65",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.87",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.58",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","704.87",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'173",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","745.52",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","393'632",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8217 fused and 6391 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4268"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.36",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.51",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.81"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","520.50",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","669'696",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'013.41",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'054'362",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'640.43",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","868'704",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'013.41",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'054'362",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'871.30",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'217'448",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.08% above the average, while the minimum instance value is 44.51% below the average.","global","17.76"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.03% above the average, while the minimum instance value is 44.31% below the average.","global","16.91"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.08% above the average, while the minimum instance value is 44.51% below the average.","global","17.76"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 31.27% above the average, while the minimum instance value is 17.12% below the average.","global","16.04"
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'199",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"16","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5936 excessive sectors (21% of the total 28764 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.58"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'022",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.64",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.71",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.28",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.85",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.24",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'004.02",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.20",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.83",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.83",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.16"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","149.04",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.64",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.10",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.79",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.16",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.20",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.908"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.977"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.67",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.33",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.90",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.90 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.33"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.04",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.08",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.92",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.63",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","701.09",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'173",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","745.77",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","393'768",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8073 fused and 6279 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4206"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.63",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.32",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.52"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","512.33",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","662'528",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'004.02",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'066'514",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'585.77",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","855'648",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'004.02",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'066'514",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'023.52",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'266'056",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.24% above the average, while the minimum instance value is 41.51% below the average.","global","17.56"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 45.25% above the average, while the minimum instance value is 44.70% below the average.","global","16.93"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.24% above the average, while the minimum instance value is 41.51% below the average.","global","17.56"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.43% above the average, while the minimum instance value is 16.83% below the average.","global","9.483"
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'959",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.40",
"17","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5656 excessive sectors (20% of the total 28075 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.37"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'419",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.43",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.59",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.54",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.27",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.18",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'938.36",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.89",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.64",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.64",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.79"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","142.98",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.43",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.90",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.41",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.31",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.89",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.832"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.821"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.29",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.71",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.83",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.83 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.71"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.86",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.78",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.93",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.64",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","700.85",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'048",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","753.47",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","397'833",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8064 fused and 6272 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4295"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.17",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.03",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.99"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","515.33",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","689'408",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'938.36",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'102'598",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'534.24",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","896'640",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'938.36",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'102'598",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'978.77",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'410'392",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.87% above the average, while the minimum instance value is 42.28% below the average.","global","16.49"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.10% above the average, while the minimum instance value is 45.92% below the average.","global","17.15"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.87% above the average, while the minimum instance value is 42.28% below the average.","global","16.49"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.84% above the average, while the minimum instance value is 17.21% below the average.","global","6.232"
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'944",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.39",
"18","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5829 excessive sectors (21% of the total 28315 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.994"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'180",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.35",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.66",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.96",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.29",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'985.05",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.79",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.01",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.01",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.04"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","146.76",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.35",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.85",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.11",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.10",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.79",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.783"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.764"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.46",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.54",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.26",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.26 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.54"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.44",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.49",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.91",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.62",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","702.03",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'673",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","746.44",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","394'120",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8109 fused and 6307 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4252"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.27",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.46",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.9"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","513.67",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","673'792",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'985.05",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'114'096",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'680.64",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","871'488",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'985.05",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'114'096",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'932.22",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'456'384",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.53% above the average, while the minimum instance value is 41.98% below the average.","global","16.46"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.13% above the average, while the minimum instance value is 47.92% below the average.","global","17.07"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.53% above the average, while the minimum instance value is 41.98% below the average.","global","16.46"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.00% above the average, while the minimum instance value is 15.72% below the average.","global","9.28"
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'019",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.41",
"19","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5671 excessive sectors (20% of the total 28177 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.38"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'061",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.38",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.65",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.79",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.15",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'004.52",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.83",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.57",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.57",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.24"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","145.78",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.38",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.84",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.18",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.85",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.83",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.789"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.785"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.96",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.04",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.05",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.05 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.04"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.24",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.19",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.98",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.70",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","696.35",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","367'673",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","738.31",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","389'826",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7893 fused and 6139 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4112"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.22",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.06",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.93"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","504.17",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","663'168",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'004.52",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'106'108",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'756.49",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","861'216",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'004.52",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'106'108",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'958.39",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'424'432",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.61% above the average, while the minimum instance value is 42.35% below the average.","global","17.07"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.85% above the average, while the minimum instance value is 45.00% below the average.","global","16.89"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.61% above the average, while the minimum instance value is 42.35% below the average.","global","17.07"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 15.61% above the average, while the minimum instance value is 14.14% below the average.","global","8.274"
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'659",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.32",
"20","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5448 excessive sectors (20% of the total 27372 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.55"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.58",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'172",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.58",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.79",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.74",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.38",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'084.35",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.99",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.27",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.97",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.27",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.48"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","150.30",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.58",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.01",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.08",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.63",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.99",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.895"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.936"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.72",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.28",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.25",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.25 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.28"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.09",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.88",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.82",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.52",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","709.13",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","374'423",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","748.66",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","395'290",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8379 fused and 6517 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4252"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.34",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.14",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.81"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","529.17",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","670'208",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'084.35",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'097'396",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'671.28",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","870'816",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'084.35",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'097'396",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'910.88",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'389'584",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.47% above the average, while the minimum instance value is 42.52% below the average.","global","16.87"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 48.25% above the average, while the minimum instance value is 47.75% below the average.","global","16.89"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.47% above the average, while the minimum instance value is 42.52% below the average.","global","16.87"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 20.31% above the average, while the minimum instance value is 15.76% below the average.","global","10.46"
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'469",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.53",
"21","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5993 excessive sectors (20% of the total 29324 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.52"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'395",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.40",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.58",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.50",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.24",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.26",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'923.68",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.95",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.35",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.01",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.35",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.73"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","143.58",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.40",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.90",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.93",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.72",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.95",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.779"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.803"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.43",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.57",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.35",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.35 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.57"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.84",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.81",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.95",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.67",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","699.19",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","369'173",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","741.13",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","391'316",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8001 fused and 6223 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4283"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.96",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.89",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.2"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","514.50",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","689'024",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'923.68",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'099'298",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'606.71",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","895'488",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'923.68",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'099'298",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'914.21",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'397'192",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.84% above the average, while the minimum instance value is 42.16% below the average.","global","16.79"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.41% above the average, while the minimum instance value is 48.70% below the average.","global","17.29"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.84% above the average, while the minimum instance value is 42.16% below the average.","global","16.79"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.05% above the average, while the minimum instance value is 17.19% below the average.","global","13.36"
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'839",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.37",
"22","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5468 excessive sectors (20% of the total 27767 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.725"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'501",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.67",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.61",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.57",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.88",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.22",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'198.38",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.23",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.51",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.94",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.51",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.13"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","145.10",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.67",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.13",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.24",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.08",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.23",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.931"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.63",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.37",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.54",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.54 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.37"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.32",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.54",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.88",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.58",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","704.87",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'173",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","751.88",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","396'991",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8217 fused and 6391 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4021"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.18",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.96",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.93"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","526",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","699'904",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'198.38",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'068'654",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'534.47",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","906'912",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'198.38",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'068'654",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'933.53",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'274'616",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.46% above the average, while the minimum instance value is 39.56% below the average.","global","17.17"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.80% above the average, while the minimum instance value is 44.91% below the average.","global","16.96"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.46% above the average, while the minimum instance value is 39.56% below the average.","global","17.17"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 24.55% above the average, while the minimum instance value is 23.43% below the average.","global","11.78"
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'199",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"23","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5882 excessive sectors (20% of the total 28770 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.813"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'180",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.49",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.62",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.93",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.17",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'987.04",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.01",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.85",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.85",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.1"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","145.48",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.49",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.96",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.88",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.51",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.01",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.828"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.87"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.69",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.31",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.07",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.07 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.31"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.41",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.35",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.95",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.66",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","699.19",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","369'173",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","742.33",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","391'950",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8001 fused and 6223 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4192"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.69",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.36",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.46"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","509.17",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","674'816",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'987.04",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'088'620",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'561.22",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","872'064",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'987.04",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'088'620",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'889.65",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'354'480",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.33% above the average, while the minimum instance value is 41.82% below the average.","global","16.42"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 48.63% above the average, while the minimum instance value is 48.30% below the average.","global","17.04"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.33% above the average, while the minimum instance value is 41.82% below the average.","global","16.42"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.36% above the average, while the minimum instance value is 20.20% below the average.","global","11.23"
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'839",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.37",
"24","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5514 excessive sectors (20% of the total 27778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.967"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'800",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.44",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.80",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.12",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.40",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.54",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'253.36",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.97",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.86",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.57",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.57",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.53"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","152.65",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.44",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.91",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.72",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.31",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.97",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.84"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.832"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.09",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.91",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.26",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.26 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.91"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.33",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.50",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.01",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.72",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","695.88",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","367'423",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","734.33",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","387'724",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7875 fused and 6125 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3789"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 41.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","57.60",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.86",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (57.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","41.49"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","508.83",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","643'456",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'253.36",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'092'820",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'341.25",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","834'624",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'253.36",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'092'820",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'814.72",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'371'280",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.26% above the average, while the minimum instance value is 39.51% below the average.","global","17"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.32% above the average, while the minimum instance value is 46.39% below the average.","global","17.11"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.26% above the average, while the minimum instance value is 39.51% below the average.","global","17"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 31.00% above the average, while the minimum instance value is 24.42% below the average.","global","15.48"
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'629",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.31",
"25","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5754 excessive sectors (21% of the total 27759 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.35"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'159",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.23",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.59",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.14",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.14",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'928.89",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.64",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.41",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.41",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.85"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","143.81",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.23",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.71",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.25",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.42",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.64",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.735"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.669"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.89",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.11",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.60",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.60 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.11"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.23",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.66",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.04",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.76",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","693.51",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","366'173",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","744.20",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","392'936",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7785 fused and 6055 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.416"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.31",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.48",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.86"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","503.33",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","672'256",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'928.89",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'126'668",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'625.34",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","869'088",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'928.89",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'126'668",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'874.91",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'506'672",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.26% above the average, while the minimum instance value is 42.78% below the average.","global","16.9"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.65% above the average, while the minimum instance value is 44.03% below the average.","global","17.06"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.26% above the average, while the minimum instance value is 42.78% below the average.","global","16.9"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.84% above the average, while the minimum instance value is 12.89% below the average.","global","9.625"
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'479",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.28",
"26","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5643 excessive sectors (21% of the total 27428 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.51"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'134",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.19",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.47",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.26",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.03",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'835.39",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.73",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.51",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.51",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.49"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","137.76",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.19",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.71",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.61",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.62",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.73",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.725"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.64"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.88",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.12",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.14",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.14 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.12"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.73",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.78",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.23",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.96",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 33.9% of the total average of 32.7 cycles between issuing two instructions.","global","33.87"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","680.73",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","359'423",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","723.44",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","381'975",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7299 fused and 5677 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4029"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.88",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.84",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.28"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","482.17",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","667'264",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'835.39",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'104'850",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'540.34",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","866'976",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'835.39",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'104'850",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'907.70",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'419'400",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.48% above the average, while the minimum instance value is 41.07% below the average.","global","16.76"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.81% above the average, while the minimum instance value is 42.22% below the average.","global","16.96"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.48% above the average, while the minimum instance value is 41.07% below the average.","global","16.76"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 21.61% above the average, while the minimum instance value is 20.38% below the average.","global","10.86"
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","33'669",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.07",
"27","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5410 excessive sectors (21% of the total 25901 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.5"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'123",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.23",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.41",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.34",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.80",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'953.20",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.88",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.30",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.97",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.30",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.28"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","135.76",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.23",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.76",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.77",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.00",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.88",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.741"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.67"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.33",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.67",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.53",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.53 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.67"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.37",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.42",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.32",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.06",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.1 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 34.2% of the total average of 32.4 cycles between issuing two instructions.","global","34.16"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","675.04",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","356'423",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","717.67",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","378'928",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7083 fused and 5509 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3754"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.79",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.43",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.35"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","472.33",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","665'600",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'953.20",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'085'576",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'386.12",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","865'824",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'953.20",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'085'576",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'725.42",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'342'304",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.89% above the average, while the minimum instance value is 40.84% below the average.","global","17.56"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.32% above the average, while the minimum instance value is 45.00% below the average.","global","17.01"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.89% above the average, while the minimum instance value is 40.84% below the average.","global","17.56"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.60% above the average, while the minimum instance value is 30.67% below the average.","global","10.99"
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","33'309",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.98",
"28","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5372 excessive sectors (21% of the total 25399 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.29"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'290",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.18",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.16",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.00",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.53",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'780.61",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.99",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.43",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.43",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.43"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","126.31",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.18",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.74",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.66",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.20",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.99",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.704"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.634"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.85",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.15",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.61",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.61 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.15"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.06",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.14",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.48",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.23",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","663.92",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","350'548",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","707.06",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","373'327",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 6660 fused and 5180 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3749"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.24",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.07",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.92"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","447.33",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","678'656",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'780.61",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'069'324",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'366.58",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","883'968",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'780.61",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'069'324",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'633.56",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'277'296",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.68% above the average, while the minimum instance value is 40.19% below the average.","global","17.4"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.74% above the average, while the minimum instance value is 46.27% below the average.","global","16.82"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.68% above the average, while the minimum instance value is 40.19% below the average.","global","17.4"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.23% above the average, while the minimum instance value is 18.22% below the average.","global","11.96"
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","32'604",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.80",
"29","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4903 excessive sectors (21% of the total 23693 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.813"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'924",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.05",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.10",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.22",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.24",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.26",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'990.96",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.94",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.32",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.06",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.92",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.06",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.9"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","124.02",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.05",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.63",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.97",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.63",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.94",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.642"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.535"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.63",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.37",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.90",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.90 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.37"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.44",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.56",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.73",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.49",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","648.53",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","342'423",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","689.74",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","364'184",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 6075 fused and 4725 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3179"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 40.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","58.98",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.75",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (59.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","40.09"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","421.17",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","651'264",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'990.96",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'060'482",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'639.10",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","844'896",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'990.96",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'060'482",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'590.49",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'241'928",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.60% above the average, while the minimum instance value is 36.78% below the average.","global","18.84"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 52.21% above the average, while the minimum instance value is 40.98% below the average.","global","16.84"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.60% above the average, while the minimum instance value is 36.78% below the average.","global","18.84"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.04% above the average, while the minimum instance value is 15.61% below the average.","global","10.04"
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","31'629",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.56",
"30","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4602 excessive sectors (21% of the total 21897 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.08"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'960",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.62",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.84",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.39",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.22",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'559.11",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.44",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.99",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.30",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.42",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.06",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.42",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.47"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","113.56",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.62",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.26",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.13",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.70",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.44",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.456"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.217"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.63",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.37",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.78",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.78 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.37"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.96",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.20",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.97",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.75",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","633.14",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","334'298",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","676.18",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","357'023",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 5490 fused and 4270 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3358"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.08",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.61",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.07"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","388",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","656'384",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'559.11",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'104'818",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'289.26",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","848'352",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'559.11",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'104'818",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'539.26",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'419'272",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.13% above the average, while the minimum instance value is 35.49% below the average.","global","16.55"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 55.80% above the average, while the minimum instance value is 41.12% below the average.","global","16.93"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.13% above the average, while the minimum instance value is 35.49% below the average.","global","16.55"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.22% above the average, while the minimum instance value is 16.77% below the average.","global","11.75"
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","30'654",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.31",
"31","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4129 excessive sectors (21% of the total 19739 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.15"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'853",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.71",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.51",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.18",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.38",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.54",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'438.46",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.94",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.00",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.31",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.70",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.07",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.70",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.97"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","99.85",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.71",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.40",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.69",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.06",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.94",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.2 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.473"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.286"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.24",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.76",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.60",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.60 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.76"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.00",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.23",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.35",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.16",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","610.89",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","322'548",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","650.97",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","343'712",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 4644 fused and 3612 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2981"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.40",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.10",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.7"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","337",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","645'120",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'438.46",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'035'358",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'317.32",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","837'024",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'438.46",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'035'358",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'305.32",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'141'432",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.38% above the average, while the minimum instance value is 34.38% below the average.","global","16.91"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 56.32% above the average, while the minimum instance value is 41.70% below the average.","global","16.55"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.38% above the average, while the minimum instance value is 34.38% below the average.","global","16.91"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 31.48% above the average, while the minimum instance value is 14.18% below the average.","global","15.59"
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","29'244",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.95",
"32","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3438 excessive sectors (21% of the total 16704 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.19"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'770",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.40",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.21",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.12",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.69",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.15",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'425.51",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.67",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.97",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.30",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.86",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.86",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.26"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","88.15",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.40",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.12",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.08",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.67",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.67",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.356"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.052"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.32",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.68",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.19",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.19 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.68"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.46",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.46",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.72",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.55",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","590.76",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","311'923",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","627.20",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","331'160",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3879 fused and 3017 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2503"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.46",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.13",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.65"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","293.83",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","638'976",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'425.51",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'048'022",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'062.24",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","828'384",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'425.51",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'048'022",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'214.84",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'192'088",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.78% above the average, while the minimum instance value is 32.10% below the average.","global","17.65"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 58.99% above the average, while the minimum instance value is 33.00% below the average.","global","16.46"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.78% above the average, while the minimum instance value is 32.10% below the average.","global","17.65"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 29.57% above the average, while the minimum instance value is 27.77% below the average.","global","13.92"
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","27'969",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.63",
"33","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2956 excessive sectors (21% of the total 14042 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.91"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'796",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.02",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.93",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.15",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.63",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.74",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'220.32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.23",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.83",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.11",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.83",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (20.2%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","76.72",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.02",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.80",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.89",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.15",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.23",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.213"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.768"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.91",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.09",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.18",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.18 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.09"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.89",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.30",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.00",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.85",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","575.85",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","304'048",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","617.98",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","326'291",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3312 fused and 2576 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2335"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.95",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.44",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.15"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","257.33",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","639'488",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'220.32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'086'888",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'264.31",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","832'224",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'220.32",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'086'888",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'214.44",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'347'552",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.81% above the average, while the minimum instance value is 33.61% below the average.","global","16.13"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.12% above the average, while the minimum instance value is 33.62% below the average.","global","16.44"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.81% above the average, while the minimum instance value is 33.61% below the average.","global","16.13"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.44% above the average, while the minimum instance value is 19.94% below the average.","global","13.5"
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","27'024",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.39",
"34","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2619 excessive sectors (22% of the total 12087 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.66"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'130",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.66",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.47",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.48",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.07",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'124.86",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.91",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.87",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.11",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.87",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (20.6%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","58.67",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.66",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.49",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.78",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.12",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.91",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.058"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.496"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.90",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.10",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.11",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.11 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.1"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.02",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.66",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.43",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.32",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","554.54",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","292'798",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","592.12",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","312'638",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2502 fused and 1946 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1843"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 40.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","59.02",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.77",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (59.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","40.05"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","205.33",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","668'928",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'124.86",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'111'812",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'007.88",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","868'224",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'124.86",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'111'812",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'286.26",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'447'248",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.51% above the average, while the minimum instance value is 28.98% below the average.","global","15.52"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 60.49% above the average, while the minimum instance value is 29.75% below the average.","global","16.42"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.51% above the average, while the minimum instance value is 28.98% below the average.","global","15.52"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.28% above the average, while the minimum instance value is 33.33% below the average.","global","9.875"
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","25'674",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.05",
"35","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2000 excessive sectors (22% of the total 9158 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.678"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'987",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.66",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.26",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.69",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.88",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'279.45",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.11",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.28",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.01",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.28",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.03"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","50.39",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.66",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.52",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.97",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.78",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.11",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.076"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.495"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.10",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.90",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.08",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.08 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.9"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.30",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.30",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.69",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.59",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","542.70",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","286'548",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","576.34",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","304'306",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2052 fused and 1596 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1409"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.39",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.21",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.59"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","172.17",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","655'488",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'279.45",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'077'852",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'672.95",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","853'536",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'279.45",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'077'852",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'051.11",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'311'408",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 60.84% above the average, while the minimum instance value is 22.04% below the average.","global","16.98"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.32% above the average, while the minimum instance value is 30.92% below the average.","global","15.91"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 60.84% above the average, while the minimum instance value is 22.04% below the average.","global","16.98"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 30.25% above the average, while the minimum instance value is 26.68% below the average.","global","12.5"
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","24'924",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.86",
"36","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1695 excessive sectors (22% of the total 7568 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.252"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'688",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.74",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.96",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.06",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.05",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.36",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'017.37",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.56",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.68",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.11",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.68",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","38.33",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.74",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.63",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.46",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","43.99",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.56",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.088"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.552"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.32",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.68",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.81",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.81 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.68"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.35",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.30",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.05",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.98",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.7 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 33.0% of the total average of 32.4 cycles between issuing two instructions.","global","33.01"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.1 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 31.1% of the total average of 32.4 cycles between issuing two instructions.","global","31.07"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","526.61",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","278'048",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","558.31",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","294'788",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1440 fused and 1120 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1117"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 37.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","61.23",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.19",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (61.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","37.8"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","126.17",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","632'576",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'017.37",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'014'880",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'385.10",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","821'664",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'017.37",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'014'880",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'841.12",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'059'520",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.89% above the average, while the minimum instance value is 19.90% below the average.","global","16.24"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.19% above the average, while the minimum instance value is 24.45% below the average.","global","15.37"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.89% above the average, while the minimum instance value is 19.90% below the average.","global","16.24"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 27.06% above the average, while the minimum instance value is 45.17% below the average.","global","10.7"
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'904",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.61",
"37","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1162 excessive sectors (22% of the total 5297 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.676"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'900",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.50",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.74",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.22",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.92",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.08",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'876.95",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.27",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.51",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.51",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","29.55",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.50",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.42",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.12",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","46.30",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.27",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.009"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.377"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.10",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.90",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.55",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.55 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.9"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.72",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.95",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.27",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.21",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","517.61",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","273'298",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","553.95",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","292'486",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1098 fused and 854 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.09156"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 37.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.05",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.72",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.96"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","100.33",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","649'728",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'876.95",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'040'948",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'104.55",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","844'128",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'876.95",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'040'948",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'840.10",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'163'792",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.29% above the average, while the minimum instance value is 21.84% below the average.","global","15.3"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.80% above the average, while the minimum instance value is 23.16% below the average.","global","15.35"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.29% above the average, while the minimum instance value is 21.84% below the average.","global","15.3"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 29.36% above the average, while the minimum instance value is 39.12% below the average.","global","10.37"
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'334",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.46",
"38","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 938 excessive sectors (23% of the total 4105 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.068"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'661",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.54",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.02",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","15.88",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.82",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'175.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.09",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.93",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.93",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.86"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","21.81",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.27",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.66",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","46.98",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.09",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.894"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.245"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.81",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.19",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.77",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.77 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.19"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.91",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.19",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.50",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.46",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","508.14",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","268'298",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","542.28",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","286'324",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 738 fused and 574 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0531"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 30.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","68.84",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","44.05",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (68.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","30.07"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","71.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","631'552",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'175.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'054'054",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'803.66",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","819'264",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'175.33",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'054'054",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'882.11",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'216'216",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.50% above the average, while the minimum instance value is 15.60% below the average.","global","17.03"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.05% above the average, while the minimum instance value is 21.26% below the average.","global","15.57"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.50% above the average, while the minimum instance value is 15.60% below the average.","global","17.03"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 38.89% above the average, while the minimum instance value is 48.92% below the average.","global","12.78"
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'734",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.31",
"39","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 573 excessive sectors (21% of the total 2685 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","7.011"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'478",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.40",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.43",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.93",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.43",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.80",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'847.76",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.34",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.93",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.93",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","17.35",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.40",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.35",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.39",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","45.74",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.34",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.945"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.298"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.05",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.95",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.96",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.96 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.95"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.07",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.09",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.63",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.60",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 42.1% of the total average of 32.1 cycles between issuing two instructions.","global","42.06"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.8 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 33.8% of the total average of 32.1 cycles between issuing two instructions.","global","33.8"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","502.93",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","265'548",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","534.52",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","282'229",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 540 fused and 420 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.04574"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.82",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.21",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.18"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","55.67",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","614'400",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'847.76",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'022'102",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'634.19",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","799'776",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'847.76",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'022'102",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'721.55",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'088'408",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.42% above the average, while the minimum instance value is 18.06% below the average.","global","15.61"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.46% above the average, while the minimum instance value is 24.84% below the average.","global","15"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.42% above the average, while the minimum instance value is 18.06% below the average.","global","15.61"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.80% above the average, while the minimum instance value is 53.34% below the average.","global","12.9"
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'404",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.23",
"40","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 440 excessive sectors (22% of the total 1990 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.991"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'495",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.22",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.23",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.93",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.78",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.46",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'867.01",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.14",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.06",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.17",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.13",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.17",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.0%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","9.09",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.22",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.21",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.18",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","56.73",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.14",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.968"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.166"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.57",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.43",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.74",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.63",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.74 active warps per scheduler, but only an average of 0.63 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.43"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","37.60",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.96",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.84",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.82",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","494.88",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","261'298",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","525.93",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","277'693",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 234 fused and 182 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.01962"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.89",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.89",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.1"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","29.17",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","615'680",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'867.01",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'038'240",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'961.41",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","802'080",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'867.01",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'038'240",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'840.72",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'152'960",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.50% above the average, while the minimum instance value is 17.41% below the average.","global","15.07"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.25% above the average, while the minimum instance value is 16.01% below the average.","global","15.5"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.50% above the average, while the minimum instance value is 17.41% below the average.","global","15.07"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.73% above the average, while the minimum instance value is 88.27% below the average.","global","11.68"
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'894",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.10",
"41","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 233 excessive sectors (26% of the total 909 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.017"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'175",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.30",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.13",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.74",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","19.09",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.43",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'718.61",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.40",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.70",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.70",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","5.14",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.30",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.30",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.22",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","62.65",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.40",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.88"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.227"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.47",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.53",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.99",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.99 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.53"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.78",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.22",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.94",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.93",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 12.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 36.6% of the total average of 32.8 cycles between issuing two instructions.","global","36.6"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","491.09",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'298",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","527.68",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","278'617",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 90 fused and 70 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.008196"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.07",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.01",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.91"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","15.83",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","587'904",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'718.61",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'006'896",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'218.07",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","767'808",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'718.61",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'006'896",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'731.81",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'027'584",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.81% above the average, while the minimum instance value is 14.76% below the average.","global","14.83"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.42% above the average, while the minimum instance value is 22.91% below the average.","global","15.08"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.81% above the average, while the minimum instance value is 14.76% below the average.","global","14.83"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.03% above the average, while the minimum instance value is 82.43% below the average.","global","7.468"
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'654",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.04",
"42","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 67 excessive sectors (21% of the total 322 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","3.169"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'692",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.28",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.08",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.06",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.83",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'735.30",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.39",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.41",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.41",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.5%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","3.19",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.28",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.28",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.14",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","68.87",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.39",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.001"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.214"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.84",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.16",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.00",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.00 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.16"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.53",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.13",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.97",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.97",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.67",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'548",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","527.66",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","278'602",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 36 fused and 28 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.003247"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.20",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.09",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.78"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","10.50",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","632'704",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'735.30",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'006'650",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","845.84",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","824'544",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'735.30",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'006'650",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'768.35",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'026'600",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.69% above the average, while the minimum instance value is 14.02% below the average.","global","14.49"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.95% above the average, while the minimum instance value is 15.06% below the average.","global","15.29"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.69% above the average, while the minimum instance value is 14.02% below the average.","global","14.49"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 71.58% above the average, while the minimum instance value is 74.58% below the average.","global","7.049"
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'564",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.02",
"43","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 36 excessive sectors (26% of the total 140 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.532"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'426",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.50",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.06",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.90",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.25",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'788.19",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.87",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.12",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.12",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.56",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.50",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.50",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.64",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","63.68",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.87",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.951"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.375"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.47",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.53",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.16",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.16 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.53"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.29",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.37",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 15.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 46.8% of the total average of 32.3 cycles between issuing two instructions.","global","46.77"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 31.9% of the total average of 32.3 cycles between issuing two instructions.","global","31.94"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.20",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'298",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.67",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'915",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 18 fused and 14 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.001575"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.82",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.84",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.17"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","8.17",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","609'792",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'788.19",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","957'090",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","673.77",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","794'976",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'788.19",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","957'090",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'654.42",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'828'360",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.21% above the average, while the minimum instance value is 15.56% below the average.","global","15.84"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.78% above the average, while the minimum instance value is 17.55% below the average.","global","15.01"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.21% above the average, while the minimum instance value is 15.56% below the average.","global","15.84"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 66.78% above the average, while the minimum instance value is 68.09% below the average.","global","5.433"
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'534",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.01",
"44","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 14 excessive sectors (21% of the total 66 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.726"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","6'745",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.43",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.06",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.45",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.93",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'818.97",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.73",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.62",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.14",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.62",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.59",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.43",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.43",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.33",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","65.49",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.73",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.662"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.324"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.07",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.93",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.32",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.32 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","67.93"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.19",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.27",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.1 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 31.4% of the total average of 32.2 cycles between issuing two instructions.","global","31.42"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.51",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'829",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0007744"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.99",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.96",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.99"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","7.50",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","554'240",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'818.97",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","971'208",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","582.56",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","722'400",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'818.97",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","971'208",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'623.26",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'884'832",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.44% above the average, while the minimum instance value is 11.87% below the average.","global","15.68"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.18% above the average, while the minimum instance value is 16.59% below the average.","global","14.82"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.44% above the average, while the minimum instance value is 11.87% below the average.","global","15.68"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.94% above the average, while the minimum instance value is 63.09% below the average.","global","5.569"
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"45","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (13% of the total 30 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.032"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","6'979",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.64",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.07",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.61",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.55",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'757.34",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.19",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.11",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.83",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.83",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.2%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.61",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.64",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.64",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.33",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","40.88",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.74"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.48"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.26",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.74",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.89",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.61",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.89 active warps per scheduler, but only an average of 0.61 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.74"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.80",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.24",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","524.25",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","276'802",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0008016"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.68",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.12",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.32"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","7.83",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","576'256",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'757.34",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","927'568",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","588.42",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","747'360",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'757.34",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","927'568",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'791.81",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'710'272",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.92% above the average, while the minimum instance value is 14.53% below the average.","global","16.24"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.92% above the average, while the minimum instance value is 15.90% below the average.","global","15.79"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.92% above the average, while the minimum instance value is 14.53% below the average.","global","16.24"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.62% above the average, while the minimum instance value is 63.63% below the average.","global","5.186"
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"46","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (13% of the total 30 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.008"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'567",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.40",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.07",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.99",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.96",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.37",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'720.73",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.66",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.77",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.77",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.7%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.77",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.40",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.40",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.64",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","47.46",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.66",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.909"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.302"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.78",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.22",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.14",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.14 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.22"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.96",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.66",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.98",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.98",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.33",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'364",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","529.43",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","279'541",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 26 fused and 16 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.001929"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.89",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.25",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.11"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","9",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","625'152",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'720.73",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","978'426",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","642.55",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","810'528",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'720.73",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","978'426",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'720.30",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'913'704",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.40% above the average, while the minimum instance value is 15.79% below the average.","global","15.18"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.04% above the average, while the minimum instance value is 23.15% below the average.","global","15.33"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.40% above the average, while the minimum instance value is 15.79% below the average.","global","15.18"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.74% above the average, while the minimum instance value is 66.70% below the average.","global","5.46"
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'542",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","82.61",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.01",
"47","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 14 excessive sectors (21% of the total 66 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.614"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'237",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.50",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.08",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.77",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","15.57",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.41",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'099.83",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.85",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.34",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.01",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.34",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.58"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","3.33",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.50",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.49",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.14",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","54.56",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.85",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.101"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.374"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.28",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.72",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.38",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.73",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.38 active warps per scheduler, but only an average of 0.73 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","67.72"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.17",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.95",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.97",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.96",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 42.3% of the total average of 32.2 cycles between issuing two instructions.","global","42.33"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.6 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 33.1% of the total average of 32.2 cycles between issuing two instructions.","global","33.1"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.80",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'614",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","532.12",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","280'960",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 44 fused and 30 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 20% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.002925"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","69.19",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","44.28",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (69.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","29.71"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","10.33",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","595'456",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'099.83",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","959'482",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","856.78",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","775'680",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'099.83",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","959'482",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'648.66",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'837'928",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.61% above the average, while the minimum instance value is 12.37% below the average.","global","18.38"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.42% above the average, while the minimum instance value is 20.30% below the average.","global","15.07"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.61% above the average, while the minimum instance value is 12.37% below the average.","global","18.38"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 65.49% above the average, while the minimum instance value is 75.02% below the average.","global","6.945"
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'572",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","79.49",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.02",
"48","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 36 excessive sectors (26% of the total 140 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.727"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'591",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.28",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.12",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","4.99",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.29",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.40",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'794.73",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.35",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.14",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.14",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.87",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.28",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.27",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.22",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","60.83",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.35",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.871"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.211"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.87",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.13",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.43",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.43 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.13"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.87",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","38.17",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.91",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.91",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","491.47",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'496",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","523.04",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","276'167",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 114 fused and 76 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 20% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.008695"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.97",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.94",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.02"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","15.83",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","621'568",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'794.73",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'012'210",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'331.61",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","812'640",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'794.73",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'012'210",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'641.29",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'048'840",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.11% above the average, while the minimum instance value is 10.79% below the average.","global","15.47"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.89% above the average, while the minimum instance value is 18.84% below the average.","global","14.75"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.11% above the average, while the minimum instance value is 10.79% below the average.","global","15.47"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 54.18% above the average, while the minimum instance value is 83.85% below the average.","global","8.523"
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'678",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","80.20",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.04",
"49","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 67 excessive sectors (21% of the total 322 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","3.273"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'772",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.05",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.22",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.12",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.69",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.46",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'777.02",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.77",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.12",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.24",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.82",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.82",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","8.65",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.05",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.03",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.18",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","39.56",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.77",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.888"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.037"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.18",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.82",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.00",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.00 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.82"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.10",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.41",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.81",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.79",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 31.3% of the total average of 33.1 cycles between issuing two instructions.","global","31.34"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","495.38",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","261'562",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","529.88",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","279'778",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 266 fused and 190 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.02176"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.92",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.27",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.08"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","28.83",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","638'976",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'777.02",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'082'430",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'853.54",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","831'936",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'777.02",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'082'430",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'949.52",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'329'720",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.33% above the average, while the minimum instance value is 18.06% below the average.","global","14.37"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.94% above the average, while the minimum instance value is 17.42% below the average.","global","15.68"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.33% above the average, while the minimum instance value is 18.06% below the average.","global","14.37"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 50.59% above the average, while the minimum instance value is 87.38% below the average.","global","10.82"
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'926",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.97",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.10",
"50","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 233 excessive sectors (26% of the total 909 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","5.482"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'110",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.24",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.41",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.93",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.66",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'798.13",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.00",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.12",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.24",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.24",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","16.29",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.24",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.20",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.39",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","48.59",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.00",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.874"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.177"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.78",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.22",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.20",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.20 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.22"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.25",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.90",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.52",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.49",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","504.68",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","266'472",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","543.73",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","287'090",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 652 fused and 448 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 20% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.05096"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 37.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","61.92",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.63",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (61.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","37.1"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","56.67",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","664'832",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'798.13",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'060'552",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'565.23",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","867'552",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'798.13",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'060'552",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'825.90",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'242'208",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.39% above the average, while the minimum instance value is 17.80% below the average.","global","14.86"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.86% above the average, while the minimum instance value is 20.97% below the average.","global","15.19"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.39% above the average, while the minimum instance value is 17.80% below the average.","global","14.86"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 35.07% above the average, while the minimum instance value is 57.04% below the average.","global","9.956"
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'516",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","79.24",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.23",
"51","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 440 excessive sectors (22% of the total 1990 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.276"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'837",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.31",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.53",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.15",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.25",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.78",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'893.30",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.05",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.72",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.15",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.72",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","21.37",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.31",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.25",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.66",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","50.46",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.05",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.885"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.231"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.61",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.39",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.31",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.31 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.39"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.62",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.80",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.41",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.37",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 15.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 48.5% of the total average of 32.6 cycles between issuing two instructions.","global","48.55"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.0 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 30.7% of the total average of 32.6 cycles between issuing two instructions.","global","30.74"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","509.64",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","269'090",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","543.83",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","287'140",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 834 fused and 598 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.06425"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.39",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.57",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.61"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","71.67",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","645'120",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'893.30",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'059'124",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'793.62",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","838'368",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'893.30",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'059'124",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'720.67",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'236'496",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.59% above the average, while the minimum instance value is 20.24% below the average.","global","15.95"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.52% above the average, while the minimum instance value is 23.58% below the average.","global","14.69"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.59% above the average, while the minimum instance value is 20.24% below the average.","global","15.95"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 35.17% above the average, while the minimum instance value is 49.67% below the average.","global","11.25"
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'830",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.84",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.31",
"52","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 573 excessive sectors (21% of the total 2685 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.827"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'999",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.60",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.73",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.95",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.08",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'977.84",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.46",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.05",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.98",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.98",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.3%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","29.37",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.60",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.52",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.12",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","44.37",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.46",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.051"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.447"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.34",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.66",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.84",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.84 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.66"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.40",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.46",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.17",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.12",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 9.7 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 31.0% of the total average of 31.4 cycles between issuing two instructions.","global","31.03"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","519.23",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","274'156",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","553.33",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","292'160",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1202 fused and 880 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.09026"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.23",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.83",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.79"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","100.33",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","657'408",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'977.84",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'019'622",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'021.86",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","855'456",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'977.84",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'019'622",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'765.82",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'078'488",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.58% above the average, while the minimum instance value is 24.77% below the average.","global","16.28"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.82% above the average, while the minimum instance value is 23.77% below the average.","global","15.05"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.58% above the average, while the minimum instance value is 24.77% below the average.","global","16.28"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 27.53% above the average, while the minimum instance value is 38.98% below the average.","global","9.337"
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'438",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.13",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.46",
"53","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 938 excessive sectors (23% of the total 4105 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","7.749"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'067",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.48",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.91",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.55",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.26",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'965.76",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.04",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.56",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.14",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.56",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.7%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","36.48",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.48",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.38",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.46",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","41.22",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.04",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.976"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.362"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.86",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.14",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.69",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.69 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.14"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.56",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.63",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.92",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.85",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","528.98",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","279'302",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","561.47",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","296'456",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1592 fused and 1158 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1196"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 38.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","60.19",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","38.52",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (60.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","38.86"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","126.17",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","663'296",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'965.76",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'073'726",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'524.08",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","862'464",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'965.76",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'073'726",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'945.63",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'294'904",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.71% above the average, while the minimum instance value is 24.81% below the average.","global","15.64"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.48% above the average, while the minimum instance value is 26.19% below the average.","global","15.66"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.71% above the average, while the minimum instance value is 24.81% below the average.","global","15.64"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 28.95% above the average, while the minimum instance value is 54.97% below the average.","global","11.36"
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","24'056",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.35",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.61",
"54","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1162 excessive sectors (22% of the total 5297 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.605"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'245",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.58",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.21",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.70",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.73",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'035.19",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.97",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.07",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.73",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.15",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.73",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","48.80",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.58",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.44",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.99",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","39.02",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.97",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.042"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.439"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.70",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.30",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.58",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.58 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.3"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.39",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.79",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.54",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.44",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","545.45",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","288'000",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","584.77",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","308'756",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2228 fused and 1640 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1633"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 40.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","58.73",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.59",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (58.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","40.34"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","171.83",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","679'936",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'035.19",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'095'476",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'578.46",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","881'472",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'035.19",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'095'476",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'037.23",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'381'904",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.88% above the average, while the minimum instance value is 26.74% below the average.","global","15.67"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.08% above the average, while the minimum instance value is 28.24% below the average.","global","15.98"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.88% above the average, while the minimum instance value is 26.74% below the average.","global","15.67"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 34.87% above the average, while the minimum instance value is 25.44% below the average.","global","13.59"
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","25'100",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.95",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.86",
"55","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1695 excessive sectors (22% of the total 7568 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.729"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'977",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.88",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.50",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.06",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.07",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'169.16",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.35",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.03",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.34",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.09",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.34",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","59.95",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.88",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.72",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.77",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","41.19",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.35",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.155"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.662"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.40",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.60",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.41",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.41 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.6"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.02",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.12",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.33",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.21",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","556.54",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","293'854",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","593.08",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","313'145",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2630 fused and 1978 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1843"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 41.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","57.92",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.07",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (57.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","41.16"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","204.83",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","655'232",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'169.16",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'058'838",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'916.34",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","852'192",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'169.16",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'058'838",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'017.38",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'235'352",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.44% above the average, while the minimum instance value is 25.82% below the average.","global","17.7"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 63.55% above the average, while the minimum instance value is 32.19% below the average.","global","15.98"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.44% above the average, while the minimum instance value is 25.82% below the average.","global","17.7"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 24.90% above the average, while the minimum instance value is 29.81% below the average.","global","10.99"
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","25'802",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.20",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.05",
"56","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2000 excessive sectors (22% of the total 9158 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.635"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'090",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.06",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.86",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.56",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.62",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'350.90",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.32",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.99",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.15",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.05",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.15",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.8"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","74.55",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.06",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.84",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.90",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.66",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.32",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.227"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.793"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.74",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.26",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.79",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.79 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.26"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.70",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.62",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.81",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.66",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","579.72",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","306'094",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","614.83",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","324'632",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3560 fused and 2638 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2273"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 41.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","57.86",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.03",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (57.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","41.22"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","257.83",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","664'832",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'350.90",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'077'652",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'956.46",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","863'520",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'350.90",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'077'652",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'216.24",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'310'608",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 58.44% above the average, while the minimum instance value is 30.96% below the average.","global","16.83"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.04% above the average, while the minimum instance value is 30.47% below the average.","global","16.57"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 58.44% above the average, while the minimum instance value is 30.96% below the average.","global","16.83"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 20.28% above the average, while the minimum instance value is 31.43% below the average.","global","8.921"
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","27'272",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.72",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.39",
"57","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2619 excessive sectors (22% of the total 12087 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.531"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'119",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.10",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.12",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.30",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.00",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'342.40",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.16",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.02",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.03",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.08",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.03",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.43"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","84.79",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.10",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.85",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.07",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","30.77",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.16",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.226"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.829"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.35",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.65",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.11",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.11 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.65"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.30",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.45",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.52",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.35",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","594.76",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","314'035",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","633.14",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","334'297",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 4135 fused and 3081 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2662"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.94",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.80",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.17"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","295",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","667'008",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'342.40",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'108'156",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'300.52",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","866'016",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'342.40",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'108'156",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'315.34",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'432'624",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.95% above the average, while the minimum instance value is 29.43% below the average.","global","16.17"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.83% above the average, while the minimum instance value is 33.75% below the average.","global","16.5"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.95% above the average, while the minimum instance value is 29.43% below the average.","global","16.17"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 19.75% above the average, while the minimum instance value is 20.22% below the average.","global","9.416"
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","28'225",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.53",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.63",
"58","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2956 excessive sectors (21% of the total 14042 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.04"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'294",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.39",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.38",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.47",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.27",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.40",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'456.95",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.36",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.00",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.30",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.59",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.06",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.59",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.99"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","94.74",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.39",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.08",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.67",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.65",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.36",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.2 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.334"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.046"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.31",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.69",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.29",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.29 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.69"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.02",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.18",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.20",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.00",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","614.26",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","324'330",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","653.32",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","344'951",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 4860 fused and 3666 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3015"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.62",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.59",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.5"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","337.50",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","681'984",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'456.95",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'098'708",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'267.84",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","884'736",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'456.95",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'098'708",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'392.46",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'394'832",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.27% above the average, while the minimum instance value is 32.68% below the average.","global","16.91"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.58% above the average, while the minimum instance value is 41.11% below the average.","global","16.55"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.27% above the average, while the minimum instance value is 32.68% below the average.","global","16.91"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.91% above the average, while the minimum instance value is 30.67% below the average.","global","10.61"
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","29'460",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.09",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.95",
"59","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3438 excessive sectors (21% of the total 16704 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.531"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'638",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.51",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.62",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.70",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.25",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.88",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'721.17",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.29",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.30",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.91",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.91",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.42"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","104.81",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.51",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.17",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.08",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.00",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.29",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.408"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.135"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.27",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.73",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.71",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.71 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.73"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.93",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.96",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.78",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.56",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 31.9% of the total average of 31.9 cycles between issuing two instructions.","global","31.86"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","637.52",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","336'608",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","677.89",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","357'924",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 5770 fused and 4340 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3224"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.79",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.06",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.34"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","388.67",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","711'680",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'721.17",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'123'846",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'209.81",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","921'504",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'721.17",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'123'846",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'486.12",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'495'384",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 53.96% above the average, while the minimum instance value is 35.58% below the average.","global","17.25"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.25% above the average, while the minimum instance value is 39.95% below the average.","global","16.72"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 53.96% above the average, while the minimum instance value is 35.58% below the average.","global","17.25"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 31.11% above the average, while the minimum instance value is 22.54% below the average.","global","13.64"
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","30'934",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.20",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.31",
"60","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4129 excessive sectors (21% of the total 19739 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.174"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'978",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.73",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.08",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.73",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.23",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'743.24",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.41",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.31",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.12",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.12",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.36"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","123.27",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.73",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.33",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.99",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.02",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.41",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.504"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.297"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.66",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.34",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.62",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.62 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.34"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.22",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.09",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.62",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.38",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","651.15",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","343'809",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","688.98",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","363'784",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 6243 fused and 4767 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3505"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.48",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.87",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.65"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","421.17",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","657'408",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'743.24",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'120'756",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'858.06",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","851'136",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'743.24",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'120'756",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'911.96",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'483'024",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 52.36% above the average, while the minimum instance value is 35.22% below the average.","global","16.92"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.17% above the average, while the minimum instance value is 37.77% below the average.","global","17.21"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 52.36% above the average, while the minimum instance value is 35.22% below the average.","global","16.92"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.56% above the average, while the minimum instance value is 21.00% below the average.","global","12.36"
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","31'797",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.66",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.56",
"61","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4602 excessive sectors (21% of the total 21897 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.52"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'342",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.93",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.13",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.50",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.77",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.52",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'821.66",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.55",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.32",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.39",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.39",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.58"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","125.12",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.93",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.50",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.67",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.24",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.55",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.597"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.45"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.17",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.83",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.45",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.45 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.83"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.30",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.68",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.35",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.09",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","667.17",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","352'264",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","716.36",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","378'239",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 6868 fused and 5232 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3742"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.71",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.74",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.45"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","448.33",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","687'616",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'821.66",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'115'214",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'494.61",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","890'400",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'821.66",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'115'214",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'736.95",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'460'856",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.20% above the average, while the minimum instance value is 40.57% below the average.","global","17.1"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 53.29% above the average, while the minimum instance value is 41.21% below the average.","global","17.26"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.20% above the average, while the minimum instance value is 40.57% below the average.","global","17.1"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 24.22% above the average, while the minimum instance value is 22.24% below the average.","global","11.74"
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","32'812",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.75",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.80",
"62","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4903 excessive sectors (21% of the total 23693 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.03"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'518",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.74",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.24",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.60",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.17",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.59",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'820.03",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.09",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.30",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.63",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.63",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.4"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","129.60",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.74",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.30",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.77",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","33.00",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.09",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.526"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.303"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.13",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.87",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.23",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.23 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.87"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.76",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.93",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.21",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.95",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","677.79",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","357'875",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","722.80",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","381'641",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7259 fused and 5553 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3971"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.64",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.69",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.53"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","472.50",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","700'416",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'820.03",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'178'906",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'646.42",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","907'776",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'820.03",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'178'906",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'876.07",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'715'624",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.67% above the average, while the minimum instance value is 39.01% below the average.","global","16.32"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.66% above the average, while the minimum instance value is 44.72% below the average.","global","17.6"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 51.67% above the average, while the minimum instance value is 39.01% below the average.","global","16.32"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.54% above the average, while the minimum instance value is 15.85% below the average.","global","9.603"
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","33'485",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.60",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.98",
"63","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5372 excessive sectors (21% of the total 25399 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.39"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'203",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.29",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.43",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.90",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.95",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'881.71",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.95",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.19",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.01",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.19",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.69"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","137.04",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.29",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.82",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.58",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.19",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.95",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.769"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.715"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.08",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.92",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.68",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.68 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.92"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","37.12",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.42",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.12",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.85",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","683.35",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","360'809",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","725.83",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","383'236",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7467 fused and 5719 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4002"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.18",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.04",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.97"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","482.50",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","675'840",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'881.71",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'083'270",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'675.03",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","874'464",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'881.71",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'083'270",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'783.38",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'333'080",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.72% above the average, while the minimum instance value is 42.22% below the average.","global","17.11"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.81% above the average, while the minimum instance value is 48.66% below the average.","global","17.23"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.72% above the average, while the minimum instance value is 42.22% below the average.","global","17.11"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 28.24% above the average, while the minimum instance value is 17.71% below the average.","global","14.49"
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","33'837",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.55",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.07",
"64","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5410 excessive sectors (21% of the total 25901 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.72"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'664",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.35",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.39",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.70",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.83",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.88",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'148.39",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.85",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.47",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.94",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.47",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.98"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","135.55",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.35",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.84",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.26",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.19",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.85",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.789"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.762"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.09",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.91",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.32",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.32 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.91"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.88",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.87",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.96",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.68",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","695.63",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","367'295",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","738.98",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","390'182",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7921 fused and 6089 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3898"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.46",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.49",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.66"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","502.67",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","712'704",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'148.39",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'101'386",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'756.10",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","924'960",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'148.39",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'101'386",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'832.20",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'405'544",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.30% above the average, while the minimum instance value is 40.86% below the average.","global","17.85"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.74% above the average, while the minimum instance value is 47.00% below the average.","global","17.22"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.30% above the average, while the minimum instance value is 40.86% below the average.","global","17.85"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.42% above the average, while the minimum instance value is 16.86% below the average.","global","9.093"
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'615",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.42",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.28",
"65","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5643 excessive sectors (21% of the total 27428 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.16"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'486",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.39",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.49",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.57",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.30",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.96",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'280.56",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.83",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.85",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.84",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.91",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.84",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.63"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","140.32",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.39",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.85",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.69",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.59",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.83",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.816"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.791"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.85",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.15",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.67",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.67 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.15"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.55",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.07",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.97",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.69",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","696.75",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","367'885",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","749.12",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","395'537",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7931 fused and 6139 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3768"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 41.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","57.79",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.98",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (57.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","41.3"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","508.67",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","699'392",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'280.56",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'104'646",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'561.02",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","904'128",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'280.56",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'104'646",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'898.38",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'418'584",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 42.89% above the average, while the minimum instance value is 36.75% below the average.","global","16.81"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.40% above the average, while the minimum instance value is 46.38% below the average.","global","17.11"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 42.89% above the average, while the minimum instance value is 36.75% below the average.","global","16.81"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 19.25% above the average, while the minimum instance value is 17.83% below the average.","global","9.32"
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'685",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.17",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.31",
"66","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5754 excessive sectors (21% of the total 27759 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.04"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'279",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.46",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.58",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.91",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.16",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'985.72",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.97",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.06",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.06",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.06"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","143.67",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.46",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.94",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.87",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.08",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.97",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.814"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.845"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.70",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.30",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.16",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.16 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.3"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.01",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.28",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.91",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.62",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","700.19",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","369'701",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","748.34",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","395'121",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8065 fused and 6239 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4208"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.83",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.81",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.33"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","508.83",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","681'472",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'985.72",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'092'506",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'633.27",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","882'816",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'985.72",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'092'506",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'029.29",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'370'024",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.30% above the average, while the minimum instance value is 42.73% below the average.","global","17.06"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.34% above the average, while the minimum instance value is 43.65% below the average.","global","16.96"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.30% above the average, while the minimum instance value is 42.73% below the average.","global","17.06"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.86% above the average, while the minimum instance value is 19.69% below the average.","global","14.04"
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'903",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.20",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.37",
"67","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5514 excessive sectors (20% of the total 27778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'255",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.53",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.70",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.90",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.47",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'016.95",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.03",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.99",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.99",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.16"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","148.14",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.53",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.00",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.25",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.50",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.03",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.87"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.895"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.37",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.63",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.66",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.66 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.63"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.62",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.15",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.86",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.56",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","705.37",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'437",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","754.06",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","398'143",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8249 fused and 6399 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.427"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","53.35",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.14",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (53.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.8"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","524.67",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","680'960",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'016.95",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'092'198",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'678.20",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","879'648",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'016.95",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'092'198",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'859.29",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'368'792",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.78% above the average, while the minimum instance value is 41.27% below the average.","global","17.42"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.10% above the average, while the minimum instance value is 46.39% below the average.","global","16.97"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.78% above the average, while the minimum instance value is 41.27% below the average.","global","17.42"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 19.22% above the average, while the minimum instance value is 22.21% below the average.","global","9.811"
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'231",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.10",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"68","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5882 excessive sectors (20% of the total 28770 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.44"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'133",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.61",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.70",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.38",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.81",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.28",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'994.66",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.22",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.69",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.69",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.13"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","147.19",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.61",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.09",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.92",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.73",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.22",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.868"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.956"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.97",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.03",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.39",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.39 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.03"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.32",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.16",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.94",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.65",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.0 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 31.1% of the total average of 32.3 cycles between issuing two instructions.","global","31.07"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","699.57",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","369'371",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","739.46",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","390'436",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8025 fused and 6229 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4187"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.11",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.35",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.07"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","515.17",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","668'672",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'994.66",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'065'620",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'534.65",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","868'128",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'994.66",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'065'620",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'847.76",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'262'480",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.86% above the average, while the minimum instance value is 44.27% below the average.","global","17.38"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 47.09% above the average, while the minimum instance value is 46.77% below the average.","global","16.61"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.86% above the average, while the minimum instance value is 44.27% below the average.","global","17.38"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 31.50% above the average, while the minimum instance value is 15.85% below the average.","global","15.8"
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'863",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.07",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.37",
"69","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5468 excessive sectors (20% of the total 27767 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.875"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'235",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.50",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.76",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.97",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.37",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'206.10",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.91",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.44",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.94",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.44",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.1"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","149.51",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.50",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.96",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.09",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.01",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.91",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.857"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.872"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.13",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.87",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.56",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.56 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.87"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.68",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.63",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.82",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.52",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","709.26",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","374'489",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","751.48",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","396'779",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8387 fused and 6519 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4092"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.94",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.16",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.19"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","529.50",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","676'096",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'206.10",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'105'674",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'590.48",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","877'632",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'206.10",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'105'674",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'248.41",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'422'696",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.84% above the average, while the minimum instance value is 40.36% below the average.","global","16.78"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.69% above the average, while the minimum instance value is 41.82% below the average.","global","16.94"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 43.84% above the average, while the minimum instance value is 40.36% below the average.","global","16.78"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 20.57% above the average, while the minimum instance value is 20.12% below the average.","global","10.33"
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'477",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.02",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.53",
"70","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5993 excessive sectors (20% of the total 29324 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.26"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'540",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.30",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.46",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.60",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.44",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.98",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'890.47",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.75",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.65",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.65",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.57"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","138.51",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.30",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.79",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.16",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.36",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.75",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.755"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.727"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.07",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.93",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.63",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.63 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.93"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.40",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.63",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.98",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.69",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","696.48",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","367'739",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","741.44",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","391'480",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7901 fused and 6141 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4276"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.27",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.45",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.9"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","505",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","700'416",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'890.47",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'116'308",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'376.41",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","910'560",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'890.47",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'116'308",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'957.02",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'465'232",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.30% above the average, while the minimum instance value is 42.36% below the average.","global","16.51"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 49.40% above the average, while the minimum instance value is 45.62% below the average.","global","17.27"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.30% above the average, while the minimum instance value is 42.36% below the average.","global","16.51"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 14.37% above the average, while the minimum instance value is 21.63% below the average.","global","6.632"
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'667",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.02",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.32",
"71","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5448 excessive sectors (20% of the total 27372 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.184"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'018",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.68",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.75",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.28",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.59",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.43",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'057.27",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.27",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.39",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.39",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.44"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","149.33",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.68",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.13",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.11",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.73",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.27",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.928"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.013"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.26",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.74",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.07",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.07 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.74"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.96",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.93",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.90",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.61",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","702.16",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'739",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","745.59",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","393'673",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8117 fused and 6309 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4153"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.52",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.61",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.65"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","513.33",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","657'920",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'057.27",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'062'292",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'634.69",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","853'536",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'057.27",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'062'292",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'951.36",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'249'168",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.44% above the average, while the minimum instance value is 44.10% below the average.","global","18.02"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.28% above the average, while the minimum instance value is 44.06% below the average.","global","16.97"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.44% above the average, while the minimum instance value is 44.10% below the average.","global","18.02"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.39% above the average, while the minimum instance value is 16.52% below the average.","global","14.28"
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'027",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.02",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.41",
"72","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5671 excessive sectors (20% of the total 28177 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.49"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'097",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.64",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.71",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.71",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.32",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'029.61",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.23",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.55",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.55",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.31"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","148.46",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.64",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.10",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.43",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.20",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.23",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.928"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.984"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.21",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.79",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.53",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.53 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.79"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.54",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.54",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.93",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.64",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","700.85",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'048",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","743.86",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","392'756",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8064 fused and 6272 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4166"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.59",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.66",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.57"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","516.50",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","667'648",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'029.61",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'065'820",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'684.84",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","862'752",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'029.61",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'065'820",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'838.37",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'263'280",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.97% above the average, while the minimum instance value is 44.75% below the average.","global","17.62"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 48.62% above the average, while the minimum instance value is 46.66% below the average.","global","17.09"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.97% above the average, while the minimum instance value is 44.75% below the average.","global","17.62"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.61% above the average, while the minimum instance value is 19.83% below the average.","global","7.096"
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'944",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.39",
"73","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5829 excessive sectors (21% of the total 28315 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.73"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'128",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.23",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.68",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.24",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.17",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'935.66",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.61",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.44",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.44",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.77"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","147.45",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.23",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.74",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.79",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.70",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.61",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.731"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.674"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.43",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.57",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.87",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.87 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.57"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.87",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.14",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.92",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.63",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","701.09",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'173",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","746.72",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","394'270",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8073 fused and 6279 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4304"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.28",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.46",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.89"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","513",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","669'184",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'935.66",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'134'056",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'868.62",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","866'880",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'935.66",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'134'056",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'936.58",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'536'224",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.52% above the average, while the minimum instance value is 42.91% below the average.","global","15.9"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.64% above the average, while the minimum instance value is 44.25% below the average.","global","17.31"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.52% above the average, while the minimum instance value is 42.91% below the average.","global","15.9"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.73% above the average, while the minimum instance value is 22.94% below the average.","global","9.56"
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'959",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.40",
"74","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5656 excessive sectors (20% of the total 28075 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.86"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.53",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'119",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.65",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.75",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.51",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.36",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'096.68",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.15",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.16",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.97",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.16",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.61"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","150.55",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.65",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.08",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.25",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.46",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.15",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.936"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.985"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.54",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.46",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.46",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.46 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.46"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.14",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.18",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.87",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.58",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","704.87",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'173",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","748.28",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","395'091",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8217 fused and 6391 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4153"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.92",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.87",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.24"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","520.67",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","665'600",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'096.68",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'076'922",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'663.27",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","865'536",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'096.68",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'076'922",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'929.82",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'307'688",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.70% above the average, while the minimum instance value is 43.04% below the average.","global","18.11"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 46.51% above the average, while the minimum instance value is 44.02% below the average.","global","16.7"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.70% above the average, while the minimum instance value is 43.04% below the average.","global","18.11"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.69% above the average, while the minimum instance value is 20.23% below the average.","global","13.8"
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'199",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"75","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5936 excessive sectors (21% of the total 28764 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.67"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'903",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.75",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.87",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.18",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.88",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.62",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'053.86",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.27",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.69",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.69",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.3"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","155.41",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.75",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.17",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.33",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.71",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.27",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.96"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.063"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.98",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.02",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.32",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.32 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.02"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.04",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.04",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.79",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.50",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","709.62",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","374'680",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","753.87",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","398'043",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8404 fused and 6528 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4302"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.09",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.34",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.09"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","524.50",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","651'008",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'053.86",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'067'402",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'799.48",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","844'128",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'053.86",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'067'402",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'902.03",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'269'608",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.44% above the average, while the minimum instance value is 44.20% below the average.","global","17.92"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.31% above the average, while the minimum instance value is 48.48% below the average.","global","16.62"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.44% above the average, while the minimum instance value is 44.20% below the average.","global","17.92"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.29% above the average, while the minimum instance value is 14.30% below the average.","global","14.35"
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'500",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.05",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.53",
"76","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5809 excessive sectors (20% of the total 29021 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.93"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'267",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.63",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.69",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.32",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.23",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'122.27",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.16",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.99",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.99",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.73"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","147.72",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.63",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.09",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.37",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.08",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.16",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.928"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.972"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.35",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.65",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.49",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.49 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.65"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.21",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.20",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.87",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.58",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","705.58",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'548",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","749.13",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","395'543",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8244 fused and 6412 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4133"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.85",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.10",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.28"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","523.17",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","681'344",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'122.27",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'076'788",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'813.96",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","881'280",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'122.27",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'076'788",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'843.51",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'307'152",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.55% above the average, while the minimum instance value is 41.45% below the average.","global","17.43"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.38% above the average, while the minimum instance value is 44.82% below the average.","global","16.87"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.55% above the average, while the minimum instance value is 41.45% below the average.","global","17.43"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.61% above the average, while the minimum instance value is 18.34% below the average.","global","11.85"
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'244",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.47",
"77","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6040 excessive sectors (21% of the total 29046 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.9"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'476",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.85",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.71",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.60",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.17",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.16",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'037.51",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.36",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.20",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.01",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.20",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.11"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","147.70",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.85",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.27",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.45",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","35.51",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.36",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.015"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.141"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.47",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.53",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.63",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.63 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.53"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.88",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.18",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.71",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.41",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","716.71",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","378'423",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","765.40",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","404'131",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8667 fused and 6741 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4466"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.06",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.32",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.12"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","538.50",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","695'808",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'037.51",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'062'922",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'640.32",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","905'376",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'037.51",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'062'922",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'005.49",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'251'688",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.22% above the average, while the minimum instance value is 45.12% below the average.","global","17.43"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 45.20% above the average, while the minimum instance value is 46.30% below the average.","global","16.87"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.22% above the average, while the minimum instance value is 45.12% below the average.","global","17.43"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.72% above the average, while the minimum instance value is 17.98% below the average.","global","11.67"
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'949",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.65",
"78","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6132 excessive sectors (20% of the total 30180 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.997"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'227",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.19",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.72",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.41",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.28",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.35",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'966.26",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.46",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.32",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.44",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.44",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.86"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","148.92",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.19",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.67",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.98",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.28",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.46",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.71"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.639"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.07",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.93",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.64",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.47",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.64 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","77.93"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.62",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.95",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.84",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.54",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","707.00",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","373'298",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","754.49",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","398'372",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8298 fused and 6454 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4378"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.61",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.67",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.56"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","524.33",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","675'840",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'966.26",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'157'516",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'565",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","876'480",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'966.26",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'157'516",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'418.65",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'630'064",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.49% above the average, while the minimum instance value is 43.87% below the average.","global","16.4"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 45.05% above the average, while the minimum instance value is 41.03% below the average.","global","17.56"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.49% above the average, while the minimum instance value is 43.87% below the average.","global","16.4"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.02% above the average, while the minimum instance value is 14.33% below the average.","global","12.01"
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'334",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.49",
"79","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 5767 excessive sectors (20% of the total 28779 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.02"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'484",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.78",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.90",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.60",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.90",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.42",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'148.83",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.13",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.66",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.66",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.54"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","154.83",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.78",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.19",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.50",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.32",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.13",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.965"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.083"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.65",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.35",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.26",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.26 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.35"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.18",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.31",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.57",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.25",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","728.31",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","384'548",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","776.46",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","409'970",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9108 fused and 7084 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4527"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.85",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.19",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.32"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","564.50",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","695'296",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'148.83",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'098'064",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'517.27",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","904'608",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'148.83",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'098'064",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'026.62",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'392'256",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.81% above the average, while the minimum instance value is 45.09% below the average.","global","17.72"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.84% above the average, while the minimum instance value is 49.98% below the average.","global","17.04"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.81% above the average, while the minimum instance value is 45.09% below the average.","global","17.72"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.81% above the average, while the minimum instance value is 20.64% below the average.","global","10.93"
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'684",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.83",
"80","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6300 excessive sectors (20% of the total 31557 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.57"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'992",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.57",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.01",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.57",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.59",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'161.69",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.90",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.34",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.07",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.07",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.72"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","160.54",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.57",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.01",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.26",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.02",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.90",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.894"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.925"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.46",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.54",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.89",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.89 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.54"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.25",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.06",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.65",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.35",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","720.50",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","380'423",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","761.08",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","401'850",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8811 fused and 6853 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4362"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.94",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.24",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.24"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","548.50",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","656'896",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'161.69",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'116'888",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'517.07",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","851'904",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'161.69",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'116'888",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'111.81",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'467'552",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.59% above the average, while the minimum instance value is 45.76% below the average.","global","16.66"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.72% above the average, while the minimum instance value is 47.10% below the average.","global","17.18"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.59% above the average, while the minimum instance value is 45.76% below the average.","global","16.66"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 28.69% above the average, while the minimum instance value is 21.17% below the average.","global","14.6"
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'189",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.71",
"81","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6164 excessive sectors (20% of the total 30483 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.29"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'011",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.82",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.12",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.24",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.84",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'289.02",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.13",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.72",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.72",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.22"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","165.32",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.82",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.20",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.45",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.51",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.13",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.008"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.114"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.74",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.26",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.73",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.73 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.26"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.29",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.74",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.55",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.23",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","729.73",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","385'298",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","780.29",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","411'991",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9162 fused and 7126 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.436"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.40",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.82",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.73"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","564.83",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","657'408",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'289.02",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'097'950",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'552.19",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","852'864",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'289.02",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'097'950",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'154.44",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'391'800",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.86% above the average, while the minimum instance value is 41.23% below the average.","global","15.76"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.02% above the average, while the minimum instance value is 46.61% below the average.","global","16.69"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.86% above the average, while the minimum instance value is 41.23% below the average.","global","15.76"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.37% above the average, while the minimum instance value is 20.54% below the average.","global","13"
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'774",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.86",
"82","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6541 excessive sectors (20% of the total 31954 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.49"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'305",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.71",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.95",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.47",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.99",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.76",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'300.72",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.07",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.43",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.94",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.43",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.32"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","157.66",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.71",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.14",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.01",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.62",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.07",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.941"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.032"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.31",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.69",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.34",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.34 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.69"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.95",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.08",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.58",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.27",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","726.65",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","383'673",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","773.44",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","408'376",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9045 fused and 7035 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4289"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.30",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.39",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.82"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","561.67",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","683'392",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'300.72",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'103'106",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'657.30",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","886'944",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'300.72",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'103'106",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'055.31",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'412'424",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.10% above the average, while the minimum instance value is 40.65% below the average.","global","16.63"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.52% above the average, while the minimum instance value is 49.10% below the average.","global","17.01"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.10% above the average, while the minimum instance value is 40.65% below the average.","global","16.63"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 14.89% above the average, while the minimum instance value is 17.89% below the average.","global","7.505"
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'579",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.81",
"83","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6240 excessive sectors (20% of the total 31250 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.07"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'971",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.71",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.26",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.81",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.29",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'212.95",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.92",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.38",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.38",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.71"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","169.61",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.71",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.13",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.71",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.17",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.92",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.924"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.035"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.25",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.75",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.16",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.16 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.75"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.31",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.25",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.43",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.11",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","738.73",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","390'048",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","783.23",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","413'548",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9504 fused and 7392 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.463"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.54",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.62",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.63"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","579.50",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","652'544",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'212.95",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'125'190",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'995.39",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","850'080",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'212.95",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'125'190",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'102.35",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'500'760",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.40% above the average, while the minimum instance value is 43.39% below the average.","global","15.61"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 46.64% above the average, while the minimum instance value is 49.88% below the average.","global","16.97"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.40% above the average, while the minimum instance value is 43.39% below the average.","global","15.61"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.33% above the average, while the minimum instance value is 15.76% below the average.","global","10.9"
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'344",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4",
"84","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6401 excessive sectors (20% of the total 32692 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.05"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'129",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.79",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.13",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.29",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.98",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'296.32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.04",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.97",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.97",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.17"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","165.22",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.79",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.19",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.61",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.95",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.04",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.95"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.089"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.35",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.65",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.94",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.94 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.65"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.33",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.64",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.47",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.15",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","735.89",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","388'548",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","790.05",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","417'144",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9396 fused and 7308 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4461"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.37",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.88",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.82"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","574.83",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","668'160",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'296.32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'108'786",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'491.56",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","864'864",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'296.32",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'108'786",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'116.20",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'435'144",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.68% above the average, while the minimum instance value is 44.94% below the average.","global","16.75"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 45.48% above the average, while the minimum instance value is 49.33% below the average.","global","16.87"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.68% above the average, while the minimum instance value is 44.94% below the average.","global","16.75"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 29.70% above the average, while the minimum instance value is 14.62% below the average.","global","14.81"
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'164",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.95",
"85","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6322 excessive sectors (20% of the total 32346 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.744"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'107",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.09",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.23",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.31",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.81",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.99",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'238.83",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.44",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.34",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.97",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.34",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.79"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","169.83",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.09",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.46",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.29",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.63",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.44",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.099"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.316"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.27",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.73",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.05",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.05 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.73"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.15",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.19",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.39",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.06",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","742.52",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","392'048",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","788.19",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","416'165",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9648 fused and 7504 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4662"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.88",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.20",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.3"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","587.33",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","667'008",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'238.83",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'074'272",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'581.44",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","864'960",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'238.83",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'074'272",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'247.56",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'297'088",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.10% above the average, while the minimum instance value is 47.11% below the average.","global","17.15"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.79% above the average, while the minimum instance value is 48.95% below the average.","global","16.68"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 43.10% above the average, while the minimum instance value is 47.11% below the average.","global","17.15"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.47% above the average, while the minimum instance value is 18.15% below the average.","global","13.46"
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'584",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.06",
"86","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6640 excessive sectors (20% of the total 33384 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.11"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'013",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.96",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.35",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.28",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.23",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.26",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'210.89",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.10",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.70",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.70",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.57"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","173.28",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.96",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.30",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.54",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.62",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.10",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.036"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.217"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.79",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.21",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.32",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.32 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.21"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.57",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.59",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.33",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.00",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","747.96",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","394'923",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","792.93",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","418'667",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9855 fused and 7665 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4804"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.39",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.89",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.79"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","595.67",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","657'920",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'210.89",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'110'740",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'642.23",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","854'592",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'210.89",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'110'740",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'198.31",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'442'960",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.91% above the average, while the minimum instance value is 46.62% below the average.","global","16.37"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 44.72% above the average, while the minimum instance value is 50.72% below the average.","global","17"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.91% above the average, while the minimum instance value is 46.62% below the average.","global","16.37"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.17% above the average, while the minimum instance value is 17.86% below the average.","global","13.65"
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","37'929",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.15",
"87","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6793 excessive sectors (20% of the total 34163 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.37"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'420",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.65",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.54",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.54",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.93",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.51",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'467.49",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.68",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.38",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.86",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.86",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.34"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","180.76",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.65",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.84",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.22",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.79",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.68",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.363"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.734"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.11",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.89",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.54",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.54 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.89"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.01",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.02",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.95",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.59",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","781.10",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","412'423",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","827.32",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","436'826",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11115 fused and 8645 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5017"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.91",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.58",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.28"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","651.50",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","689'408",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'467.49",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'073'630",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'788.02",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","898'176",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'467.49",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'073'630",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'294.49",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'294'520",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.01% above the average, while the minimum instance value is 49.42% below the average.","global","17.06"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.54% above the average, while the minimum instance value is 52.80% below the average.","global","16.42"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.01% above the average, while the minimum instance value is 49.42% below the average.","global","17.06"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 28.34% above the average, while the minimum instance value is 18.63% below the average.","global","14.51"
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'029",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.68",
"88","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7785 excessive sectors (20% of the total 38520 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.34"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'110",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.60",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.63",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.34",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.77",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.53",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'435.59",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.81",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.39",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.07",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.07",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.25"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","185.29",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.60",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.88",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.84",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.22",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.81",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.345"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.701"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.94",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.06",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.87",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.87 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.06"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.56",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.59",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.99",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.64",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 9.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 31.3% of the total average of 31.6 cycles between issuing two instructions.","global","31.29"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","777.08",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","410'298",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","827.01",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","436'663",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 10962 fused and 8526 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4994"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.35",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.86",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.83"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","644.67",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","667'648",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'435.59",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'060'380",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'645.78",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","864'480",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'435.59",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'060'380",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'316.18",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'241'520",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 38.81% above the average, while the minimum instance value is 47.90% below the average.","global","16.6"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.13% above the average, while the minimum instance value is 49.70% below the average.","global","16.15"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 38.81% above the average, while the minimum instance value is 47.90% below the average.","global","16.6"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.72% above the average, while the minimum instance value is 14.59% below the average.","global","13.79"
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","39'774",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.61",
"89","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7696 excessive sectors (20% of the total 38040 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.44"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","7'948",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.33",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.86",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.25",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.15",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.87",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'435.58",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.25",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.92",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.52",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.52",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.06"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","193.22",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.33",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.61",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.99",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.82",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.25",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.236"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.497"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.69",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.31",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.09",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.09 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.31"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.76",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.89",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.80",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.44",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","791.05",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","417'673",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","842.30",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","444'733",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11493 fused and 8939 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5236"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.75",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.48",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.44"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","660.17",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","651'520",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'435.58",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'122'654",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'712.41",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","847'872",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'435.58",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'122'654",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'411.73",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'490'616",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.07% above the average, while the minimum instance value is 49.91% below the average.","global","16.59"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.30% above the average, while the minimum instance value is 51.46% below the average.","global","16.57"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.07% above the average, while the minimum instance value is 49.91% below the average.","global","16.59"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.01% above the average, while the minimum instance value is 14.84% below the average.","global","9.074"
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'659",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.84",
"90","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7810 excessive sectors (20% of the total 39229 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.62"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'627",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.34",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.45",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.70",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.77",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.21",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'498.60",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.29",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.05",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.05",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.38"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","176.99",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.34",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.63",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.77",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.59",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.29",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.246"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.508"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.31",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.69",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.12",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.12 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.69"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.40",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.63",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.83",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.47",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","788.68",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","416'423",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","841.42",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","444'271",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 11403 fused and 8869 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5101"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 49.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","49.58",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","31.73",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (49.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","49.63"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","656.33",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","708'096",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'498.60",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'117'342",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'530.73",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","920'640",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'498.60",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'117'342",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'461.46",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'469'368",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.23% above the average, while the minimum instance value is 48.12% below the average.","global","16.21"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.96% above the average, while the minimum instance value is 55.39% below the average.","global","16.34"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 39.23% above the average, while the minimum instance value is 48.12% below the average.","global","16.21"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 21.41% above the average, while the minimum instance value is 19.79% below the average.","global","10.11"
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","40'509",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","4.80",
"91","180348","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 7785 excessive sectors (20% of the total 38969 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.438"
